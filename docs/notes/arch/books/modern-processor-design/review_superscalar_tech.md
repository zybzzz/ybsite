# Review of SuperScalar Technology

## 总览

本章讲述的是流水线为了支持超标量的一些技术，为了打成超标量的流水线和和保证流水线上超高的效率，流水线的前端和后端都需要采用相关的手段进行优化。所谓对于前端的优化是保证指令能够源源不断的填充到流水线上，所谓对于后端的优化是保证高效的执行单元能够将前端产生的指令消耗掉。

超标量流水线上采取的技术大体可以如下分类：

- 针对指令流的优化技术
  - 分支预测
- 针对寄存器数据流的优化技术
  - 寄存器重命名
  - 特定的功能单元
  - 融汇贯通：Tomasulo
- 针对内存数据流的优化技术
  - 专用的存储器单元
  - 旁路与前递

## 针对指令流的优化技术

这部分的技术工作在取指/译码阶段，这部分技术的目标就是最大限度的向超标量流水线提供指令。

### 程序控制流和控制依赖

程序的执行并不是一帆风顺的，在遇到分支的时候总是会带来程序下一步取指的不确定性。放任不管存在以下两种情况：

1. 在分支没出结果之前停滞，等到分支出了结果继续取指。这带来了流水线上的**停顿**，但是实际上对于流水线技术而言，能够消除的停顿都要尽量的消除。
2. 在分支没出结果之前假定程序执行流不改变，默认程序继续往下。虽然不会带来取指的停止，但是如果最终发生了分支，流水线上就需要全部被清空，带来的损失依旧不小。

::: details 停顿 vs 清空
清空带来的开销更大于停顿，清空表示流水线上前几个阶段都被放弃了，就跟什么都没做一样，在这点上已经持平了停顿带来的开销。在流水线清空之前：

1. 错误的指令很可能已经被分派到保留站之中了，还要去保留站中找出具体的指令来清除。
2. 分支预测错误时，分支指令离开重排序缓冲的时候，重排序缓冲后续的条目都要被丢弃。

就这两点的开销而言就已经大于停顿。
:::

### 分支带来的性能下降

在不执行分支预测的情况下，分支的开销主要来源于停顿。分支的延迟主要包括两个部分，一部分是计算目的跳转地址，另一部分是分支条件是否满足的解析。对于第一部分的延迟，分支根据寻址方式（pc相对、寄存器相对、寄存器相对偏移）的不同主要产生 1-3 个时钟周期的延迟。对于条件解析，这部分延迟时间至少都要为 2 个周期（至少需要访问控制状态寄存器）。

### 分支预测技术

分支预测技术主要由对于目的地址的预测和对于分支条件的预测（预测是否进行跳转）组成。

![BTB](./images/ch5/btb.png)

对于目的地址的预测，引入的是 BTB 这种 kv-cache，将分支指令的 pc 作为 key，将分支指令的预测目标地址作为 value。在取指阶段的时候，就能够通过访问 BTB 来得到预测的分支目标地址。分支指令第一次访问 BTB 的时候，BTB 可能还是空的，这就需要在本次分支完成之后进行填充。BTB 怎么去做功能上的实现应该依具体的算法不同而异，重要的是知道有这么个东西来实现对于分支目标地址的查找。

对于分支预测而言更为重要的是对分支条件判断的预测，也就是到底分支不分支，对于这部分的技术其实是逐步演进的。

最简单的是对单条指令的静态分支预测，可以有如下的方案：

1. 直接根据指令中的 offset 字段判断分支是否发生。offset为正分支不跳转，offset为负分支跳转。这种方案非常的简单，但是简单的方案预测的精准度肯定还是不太够。
2. ISA 中为分支指令加一个 bit，让编译器编译的时候设置这个 bit 来预测分支是否发生。这增加了编译器设计的难度并且要修改 ISA。

静态的分支预测并不能满足要求。在超标量流水线的环境下，基于历史的分支预测是更加受欢迎的选择。

基于历史的分支预测，简单而言就是根据这条指令先前的分支结果，来预测未来的分支是否发生，这就需要考虑两个问题：

1. 考虑多久之前的历史？太多的历史导致记录的信息多，太少的历史预测精准度不高。
2. 根据历史怎么产生预测结果。这直接影响了分支预测的精准度。

对于这种分支预测的实现是使用有限确定性状态机（FSM）实现的。考虑该分支之前前 $n$ 条分支的历史，前 $n$ 分支的历史根据先前的分支跳转(Taken)与否(Not Taken)，会生成 $2^n$ 种可能，这些可能组成了 FSM 的所有状态。每种状态都会有一种先前规定好的预测结果（这是分支预测算法规定的），在取到分支指令之后，就能快速的根据这个分析结果预测是否跳转。在分支指令完成了之后，还需要将最终的分支结果与原先预测的结果进行比对，不管预测结果正确与否都会**产生状态机的状态转移**，如果预测错误还会导致流水线的清空。分支预测也就是通过这种不断的访问状态机、更新状态机完成的。

![基于历史的分支预测](./images/ch5/history_fsm_2bit.png)

如上图所示，可以将分支历史这 $n$ bits 放到 BTB 中，在取指阶段取到指令之后，立即访问 FSM 中的这 $n$ bits 状态来预测分支是否发生。通过实验数据表明，仅使用 1 位历史数据，基于历史数据的动态分支预测就达到了 $79.7\%$ 的预测精度；使用 2 个历史位时，六种工作负载的准确率从 $83.4\%$ 到 $97.5\%$ 不等。但是这并不意味着参考越多的历史就能够获得越高的精准度，后续的实验也表明增加位数到 3 位相对于 2 位而言就不再有明显的精度提升。

这之中还有另一项研究就是确定 2 bits 的预测器比较优秀的情况下，该如何设计这个 2 bits 预测器的问题。即使是两个 bits 的预测器，设计的可能性有 $2^20$ 种，这么多种选择中哪一种是最优的是一个研究的问题。相关的研究针对不同的 benchmark 对最优的 2 bits 分支预测期展开研究，结果显示，找到的最优的预测器预测成功率从 $87.1\%$ 到 $97.2\%$ 不等。

另一方面思考的是引入分支历史之后硬件上的开销。最简单的来想，硬件上的开销也就是多出了这 2 bits 的分支历史表示，具体的大小应该是 $BTB Table Size * 2$ bits。

### 分支预测失败时候的恢复

分支预测失败时候的恢复简单的来说就是对流水线进行清空，这是通过重排序缓存来实现的。在重排序缓存中，排在当前预测指令之后的指令都需要被清空。可能出现的特殊情况是，一条分支指令进行跳转之后，后续仍然带来了一系列的分支指令，这时候重排序缓冲上可能就分布着一系列的分支指令，这时候非常重要的是确定哪条分支指令预测错误，这条错误指令以及之后的指令被清空。进行区别的方式包括为分支指令加上 tag 等等的方式。

### 更先进的分支预测技术

更先进的分支预测技术是考虑了更多程序运行时上下文的预测技术。这里首先先引入的是两个概念，分别是静态指令预测(static instruction prediction)和动态指令预测(dynamic instruction prediction)，这不同于静态分支预测和动态分支预测。静态指令预测表示预测的时候只考虑当前这条指令，而不考虑程序的其他信息；动态指令预测指的是在预测的时候不止考虑当前这条分支指令，还会考虑到之前其他分支指令的执行结果，也就是说包含了更多有关程序上下文的信息。

首先介绍的是 Yeh and Patt 在 1991 年提出的两级适应性的方案，在这个方案中引入了新的硬件结构 branch history shift register (BHSR) 和 pattern history table (PHT)，BHSR 实际上直接保存了最近分支是否发生的信息，利用这个寄存器中的数值作为 key，取出在 PHT 中的 value，这个 value 是 FSM 的一个状态，通过这个状态访问 FSM 能够取得分支预测的结果。

![two level adaptive](./images/ch5/2_level_adaptive.png)

这种分支预测方案如图所示，取得了优于 $95\%$ 的精准度。这套方案也成为了一种基座，衍生出了后续的其他分支预测方案。

基于这一个基座，可以这样去描述分支预测的设计空间：

- BHSR 实现
  - global(G)：设置容量为 k 位的**单** BHSR 来跟踪 1-k 条指令的前 k 次历史。
  - individual(P)：设置容量为 k位的**一组** BHSR，BHSR 的选取是由分支指令的 pc 地址决定的。
- PHT 实现
  - global(g)：设置**单** PHT 来处理分支预测。
  - individual(p)：设置**一组** PHT 来处理分支预测。
  - shared(s):设置**一组** PHT 来处理分支预测，多个 BHSR 可能映射到一个 PHT。
- FSM 实现
  - adaptive(A)：是否采用自适应(Adaptive)的 FSM，即 FSM 能够根据分支预测的历史来改变自身的形态(即从一种状态机变成另一种不同的状态机)。

![表示示例](./images/ch5/2level_example.png)

以上是一些表示法的示例，在实际的设计中，可以选择不同的方案进行试验，得到最后的实行方案。


