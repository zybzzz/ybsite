# 大语言模型检索与记忆增强技术总结报告

## 1. 检索增强生成（RAG）的架构演进

RAG 技术已经从简单线性流程演进为更复杂的认知架构，主要包括：

### Vanilla RAG（基础版）

- 标准“检索-增强-生成”流程。
- 将外部文档分块向量化，基于语义相似度召回相关片段。
- 将召回片段作为上下文（Context）输入模型。

### Agentic RAG（代理式）

- 引入多轮思考循环。
- 模型可主动拆解任务并发起多次定向检索。
- 支持自反思与纠错（Self-RAG）。

### GraphRAG（图谱增强）

- 结合知识图谱与向量搜索。
- 利用实体间结构化关系解决全局性、总结性问题。
- 弥补传统 RAG 在跨段落关联理解上的不足。

### Repo-level RAG（仓库级）

- 面向代码等高度结构化数据。
- 借助语法树分析与调用栈拓扑实现跨文件精准检索。

## 2. 推理过程中的计算开销与 KV Cache

RAG 检索内容进入模型后会参与 Transformer 注意力计算，影响如下：

### Prefill 阶段压力

- 检索内容增加输入序列长度（Sequence Length）。
- 计算复杂度近似为 $O(N^2)$。

### KV Cache 消耗

- 为了加速 Decoding，需要在 HBM 中存储全部输入 token 的 Key/Value 矩阵。
- token 增加会线性提升 KV Cache 占用。

### Context Caching 技术

- 可持久化预计算 KV Cache。
- 相同前缀（Prefix）再次输入时可直接挂载缓存，跳过 Prefill。

## 3. 长上下文（Long Context）与 RAG 的共存逻辑

长上下文模型出现后，RAG 的必要性并未消失，两者边界如下：

### 长上下文的局限

- 虽可处理百万级 token，但全量扫描成本高。
- 极长序列仍有“中间信息丢失（Lost in the middle）”风险。

### RAG 的优势

- 支持实时更新外部知识。
- 以“按需调页”方式仅引入最相关片段，能有效控制时延与成本。

### 当前趋势

- 二者正在合流：长上下文承担大段语义建模，RAG 负责海量数据初筛。

## 4. 内部条件记忆（Engram / Conditional Memory）

以 DeepSeek 等前沿架构为代表，条件记忆强调计算与存储解耦：

### 核心机制

- 在 Transformer 隐藏层计算中，通过多头哈希（Multi-head Hashing）触发外部存储（DRAM/SSD）Embedding 索引查询。

### 注入方式

- 检索结果以特征向量注入残差连接（Residual Connection）。
- 不增加输入 token 数量。

### 技术本质

- 在不增加注意力负担（不额外消耗 KV Cache）前提下，扩展模型知识容量的稀疏化技术。

## 5. 核心结论与定性分析

| 技术维度 | 检索增强生成（RAG） | 内部条件记忆（Engram） |
| --- | --- | --- |
| 定位 | 外部咨询 / 领域专家 | 固化用户 / 特定记忆 |
| 生效阶段 | 推理前（Pre-inference） | 计算中（Intra-inference） |
| 数据形式 | 输入侧外部文本 / 数据 | 特征层注入的内部模式匹配 |
| 最佳用途 | 在核心推理前提供领域背景知识 | 在计算中固化用户记忆并提升特征提取精度 |

**总结**：RAG 的本质是在输入端通过“外部干预”缓解模型知识滞后；Engram 类技术则是在推理层通过“内部映射”扩展模型记忆容量。
